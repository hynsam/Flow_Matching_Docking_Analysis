{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ece157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct analysis of docking results and analyze molecular structure trends in order to suggest possible next steps\n",
    "#HarmonicFlow and inference are used interchangeably- analysis was run for HarmonicFlow output and with some checks/minor edits should be able to be run for other methods' output as well\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit.Chem import AllChem, GetPeriodicTable, RemoveHs\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdDistGeom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbca804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Read molecule\n",
    "#Copied from https://github.com/gcorso/DiffDock/blob/main/datasets/process_mols.py\n",
    "#Inputs\n",
    "#(a) molecule_file file with molecule\n",
    "#also can specify whether to sanitize, find charge, and remove hydrogens\n",
    "#Output\n",
    "#(a) mol the molecule\n",
    "#########\n",
    "def read_molecule(molecule_file, sanitize=False, calc_charges=False, remove_hs=False):\n",
    "    if molecule_file.endswith('.mol2'):\n",
    "        mol = Chem.MolFromMol2File(molecule_file, sanitize=False, removeHs=False)\n",
    "    elif molecule_file.endswith('.sdf'):\n",
    "        supplier = Chem.SDMolSupplier(molecule_file, sanitize=False, removeHs=False)\n",
    "        mol = supplier[0]\n",
    "    elif molecule_file.endswith('.pdbqt'):\n",
    "        with open(molecule_file) as file:\n",
    "            pdbqt_data = file.readlines()\n",
    "        pdb_block = ''\n",
    "        for line in pdbqt_data:\n",
    "            pdb_block += '{}\\n'.format(line[:66])\n",
    "        mol = Chem.MolFromPDBBlock(pdb_block, sanitize=False, removeHs=False)\n",
    "    elif molecule_file.endswith('.pdb'):\n",
    "        mol = Chem.MolFromPDBFile(molecule_file, sanitize=False, removeHs=False)\n",
    "    else:\n",
    "        raise ValueError('Expect the format of the molecule_file to be '\n",
    "                         'one of .mol2, .sdf, .pdbqt and .pdb, got {}'.format(molecule_file))\n",
    "\n",
    "    try:\n",
    "        if sanitize or calc_charges:\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "        if calc_charges:\n",
    "            # Compute Gasteiger charges on the molecule.\n",
    "            try:\n",
    "                AllChem.ComputeGasteigerCharges(mol)\n",
    "            except:\n",
    "                warnings.warn('Unable to compute charges for the molecule.')\n",
    "\n",
    "        if remove_hs:\n",
    "            mol = Chem.RemoveHs(mol, sanitize=sanitize)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"RDKit was unable to read the molecule.\")\n",
    "        return None\n",
    "\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb122b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Find rmsd w/o pytorch\n",
    "#Ref https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions\n",
    "#Ref https://github.com/HannesStark/FlowSite/blob/main/utils/train_utils.py\n",
    "#Input:\n",
    "#(a) c1 1st coords\n",
    "#(b) c2 2nd coords\n",
    "#Output: rmsd\n",
    "#########\n",
    "def rmsd_calc_no_torch(c1, c2):\n",
    "    \n",
    "    #Find difference in each atom's coordinates, find distance, square each distance\n",
    "    #Sum squares, divide by atom count, take resulting square root\n",
    "    coord_diff = c1 - c2\n",
    "    coord_dist = np.linalg.norm(coord_diff, axis = 1)\n",
    "    coord_dist_sq = coord_dist ** 2\n",
    "    sum_sq = np.sum(coord_dist_sq)\n",
    "    rmsd = (sum_sq / len(coord_diff)) ** 0.5\n",
    "    return rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Convert to scientific notation to clean\n",
    "#Copied this from https://stackoverflow.com/questions/29260893/convert-to-scientific-notation-in-python-a-×-10b\n",
    "#Input:\n",
    "#(a) number, number to convert\n",
    "#Output: \n",
    "#(b) number in scientific notation format\n",
    "#########\n",
    "def sci_notation(number, sig_fig = 3):\n",
    "    ret_string = \"{0:.{1:d}e}\".format(number, sig_fig)\n",
    "    a, b = ret_string.split(\"e\")\n",
    "    b = int(b)\n",
    "    #Ref https://stackoverflow.com/questions/21226868/superscript-in-python-plots for plot\n",
    "    return a + \"*$10^{\" + str(b) + \"}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Calculate ligand properties\n",
    "#Input:\n",
    "#(a) lcalcpdb_no_h ligand from pdb, w/o Hs\n",
    "#(b) lcalc_inf ligand from inference\n",
    "#Output:\n",
    "#(a) lprop_dict dictionary with keys of property names and values of properties\n",
    "#########\n",
    "def lprop_calc(lcalcpdb_no_h, lcalc_inf):\n",
    "    \n",
    "    #Keys are name of properties, and values are the properties\n",
    "    lprop_dict = {}\n",
    "    \n",
    "    #Heavy atom and rotatable bond count\n",
    "    #Ref https://www.rdkit.org/docs/source/rdkit.Chem.rdMolDescriptors.html\n",
    "    lprop_dict[\"Heavy_Atom_Count\"] = rdkit.Chem.rdMolDescriptors.CalcNumHeavyAtoms(lcalcpdb_no_h)\n",
    "    lprop_dict[\"Rot_Bonds\"] = rdkit.Chem.rdMolDescriptors.CalcNumRotatableBonds(lcalcpdb_no_h)\n",
    "    \n",
    "    #Radius of gyration\n",
    "    #Ref https://www.rdkit.org/docs/source/rdkit.Chem.Descriptors3D.html\n",
    "    rg_pdb = rdkit.Chem.Descriptors3D.RadiusOfGyration(lcalcpdb_no_h)\n",
    "    lprop_dict[\"Rg_PDB\"] = rg_pdb\n",
    "    \n",
    "    #Additional analyses if working with inference pose\n",
    "    if lcalc_inf is not None:\n",
    "        \n",
    "        #Check heavy atom count is equal\n",
    "        if rdkit.Chem.rdMolDescriptors.CalcNumHeavyAtoms(lcalcpdb_no_h) != rdkit.Chem.rdMolDescriptors.CalcNumHeavyAtoms(lcalc_inf):\n",
    "            print(\"GetNumHeavyAtomsISSUE, different numbers in inference vs pdb\")\n",
    "\n",
    "        #Radius of gyration percent error\n",
    "        rg_inf = rdkit.Chem.Descriptors3D.RadiusOfGyration(lcalc_inf)\n",
    "        rg_pct_error = 100.0 * (rg_inf - rg_pdb) / rg_pdb\n",
    "        lprop_dict[\"Rg_Inf\"] = rg_inf\n",
    "        lprop_dict[\"Rg_Percent_Error\"] = rg_pct_error\n",
    "    \n",
    "    return lprop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#Confirm atom ordering is correct\n",
    "#Note this is not perfect - just a check of elements, not atom names or smarts\n",
    "#Input:\n",
    "#(a) hfc HF ligand checking\n",
    "#(b) pdbc PDB ligand checking \n",
    "#######\n",
    "def check_atoms(hfc, pdbc):\n",
    "\n",
    "    #Element and atom id lists\n",
    "    hf_at_el = []\n",
    "    pdb_at_el = []\n",
    "    hf_at_id = []\n",
    "    pdb_at_id = []\n",
    "    \n",
    "    #Record atom info in each list\n",
    "    #Get ID ref https://www.rdkit.org/docs/source/rdkit.Chem.rdchem.html, https://www.rdkit.org/docs/cppapi/classRDKit_1_1Atom.html\n",
    "    #Could also explore https://www.rdkit.org/docs/source/rdkit.Chem.rdchem.html#rdkit.Chem.rdchem.Atom.GetProp\n",
    "    for hfa in hfc.GetAtoms():\n",
    "        hf_at_id.append(hfa.GetIdx())\n",
    "        hf_at_el.append(hfa.GetAtomicNum())\n",
    "    for pdba in pdbc.GetAtoms():\n",
    "        pdb_at_id.append(pdba.GetIdx())\n",
    "        pdb_at_el.append(pdba.GetAtomicNum())\n",
    "    \n",
    "    #Compare\n",
    "    for ha, pa in zip(hf_at_id, pdb_at_id):\n",
    "        if ha != pa:\n",
    "            print(\"ISSUE WITH INDEXING-IDS\")\n",
    "    for he, pe in zip(hf_at_el, pdb_at_el):\n",
    "        if ha != pa:\n",
    "            print(\"ISSUE WITH INDEXING-ELEMENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb60d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#2/1/24\n",
    "#Distance calculation- revising because will do separately for HF\n",
    "#Input:\n",
    "#(a) lfordcalc ligand\n",
    "#(b) posfordcalc positions\n",
    "#(c) a1fordcalc atom 1 for calculation\n",
    "#(d) a2fordcalc atom 2 for calculation\n",
    "#Output\n",
    "#(a) at1_atnum the atomic number of 1st atom\n",
    "#(b) at2_atnum the atomic number of 2nd atom\n",
    "#(c) calc_distance distance between atoms\n",
    "#######\n",
    "def dist_calc(lfordcalc, \n",
    "              posfordcalc, \n",
    "              a1fordcalc, \n",
    "              a2fordcalc):\n",
    "            \n",
    "    #Record elements\n",
    "    at1_atnum = lfordcalc.GetAtoms()[a1fordcalc].GetAtomicNum()\n",
    "    at2_atnum = lfordcalc.GetAtoms()[a2fordcalc].GetAtomicNum()\n",
    "\n",
    "    #Positions and distance and record\n",
    "    at_1_position = posfordcalc[a1fordcalc]\n",
    "    at_2_position = posfordcalc[a2fordcalc]\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "    calc_distance = np.linalg.norm(at_1_position - at_2_position)\n",
    "\n",
    "    return at1_atnum, at2_atnum, calc_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#Find all distances of interest\n",
    "#2/1/24 revise to call dist_calc so can work with pdb and inference/conformers separately\n",
    "#Input:\n",
    "#(a) confd conformer for distance calc\n",
    "#(b) ligd ligand for distance calc\n",
    "#(c) get_bonds_bool whether want to calculate distances from bonds\n",
    "#(d) atom_pair_list atom pairs for distances- only specify if get_bonds_bool is False\n",
    "#Output: \n",
    "#(a) d_dict distance info dictionary\n",
    "#Ref https://www.rdkit.org/docs/GettingStartedInPython.html\n",
    "#######\n",
    "def obtain_dist_data(confd, \n",
    "                     ligd, \n",
    "                     get_bonds_bool, \n",
    "                     atom_pair_list):\n",
    "    \n",
    "    if (atom_pair_list is not None) and get_bonds_bool:\n",
    "        print(\"obtain_dist_data input issue\")\n",
    "    if (atom_pair_list is None) and not get_bonds_bool:\n",
    "        print(\"obtain_dist_data input issue\")\n",
    "        \n",
    "    #Dictionary of keys of atom id pairs\n",
    "    #Values of dictionaries with distances and elements\n",
    "    d_dict = {}\n",
    "    \n",
    "    #Conformer coord pull\n",
    "    cf_positions = confd.GetPositions()\n",
    "    \n",
    "    #Assemble list of atom ids in bonds if want to work w/bonds\n",
    "    if get_bonds_bool:\n",
    "        bond_list = []\n",
    "        for b_rec in ligd.GetBonds():\n",
    "            \n",
    "            #Get atom IDs\n",
    "            at1b = b_rec.GetBeginAtomIdx()\n",
    "            at2b = b_rec.GetEndAtomIdx()\n",
    "            bond_list.append(tuple(sorted([at1b, at2b])))\n",
    "        \n",
    "        pairs_calc = bond_list\n",
    "        \n",
    "    #Otherwise work with atom pairs\n",
    "    else:\n",
    "        pairs_calc = atom_pair_list\n",
    "        \n",
    "    #Iterate over pairs\n",
    "    for ap in pairs_calc:\n",
    "        \n",
    "        #Sort and make possible dictionary entry\n",
    "        d_key = tuple(sorted([ap[0], ap[1]]))\n",
    "                      \n",
    "        #For checking make 1st entry the lower atom id\n",
    "        at1r = d_key[0]\n",
    "        at2r = d_key[1]\n",
    "        \n",
    "        #Record elements and distance if heavy atoms involved\n",
    "        an1, an2, dval = dist_calc(ligd, cf_positions, at1r, at2r)\n",
    "        if an1 > 1 and an2 > 1:\n",
    "            d_dict[d_key] = {}\n",
    "            d_dict[d_key][\"elements\"] = [an1, an2]\n",
    "            d_dict[d_key][\"distance\"] = dval\n",
    "        else:\n",
    "            print(f\"HYDROGENISSUE {an1} and {an2} atomic numbers for {at1r} and {at2r}\")\n",
    "        \n",
    "    return d_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8765640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Calculate all bonded distance percent errors\n",
    "#2/1/24 update to define set of bonds as all PDB bonds and only PDB bonds\n",
    "#Ref https://github.com/rdkit/rdkit/issues/1982\n",
    "#Ref https://www.rdkit.org/docs/GettingStartedInPython.html\n",
    "#Input:\n",
    "#(a) l_bpe_pdb ligand structure from pdb\n",
    "#(b) l_bpe_inf ligand structure from inference\n",
    "#Output:\n",
    "#(a) pct_err_list list of each bond's percent error\n",
    "#(b) pct_err_dict dictionary with bond percent error information\n",
    "#(c) avg_bond_pe average bond percent error for the ligand\n",
    "#########\n",
    "def bond_percent_error_calc(l_bpe_pdb, l_bpe_inf):\n",
    "    \n",
    "    #Confirm comparing atoms correctly\n",
    "    #Thank you (redact_for_anonymity) for also indicating in 1/30/24 conversation atom indexing should not change\n",
    "    #So while this check is not perfect, its always succeeding plus (redact_for_anonymity) remarks imply going by atom ids should be alright\n",
    "    check_atoms(l_bpe_inf, l_bpe_pdb)\n",
    "    \n",
    "    #Find all bond distances for inference and PDBbind\n",
    "    #Ref https://github.com/HannesStark/EquiBind/blob/41bd00fd6801b95d2cf6c4d300cd76ae5e6dab5e/commons/process_mols.py helped process conformers\n",
    "    distances_hf_list = []\n",
    "    distance_pdb_list = []\n",
    "    if len(l_bpe_inf.GetConformers()) > 1:\n",
    "        print(\"HF conf PBM\")\n",
    "    if len(l_bpe_pdb.GetConformers()) > 1:\n",
    "        print(\"PDB conf PBM\")\n",
    "    \n",
    "    bd_pdb = obtain_dist_data(l_bpe_pdb.GetConformers()[0], l_bpe_pdb, True, None)\n",
    "    pdb_bonds = list(bd_pdb.keys())\n",
    "    bd_hf = obtain_dist_data(l_bpe_inf.GetConformers()[0], l_bpe_inf, False, pdb_bonds)\n",
    "\n",
    "    #Find bonds in both dictionaries\n",
    "    #Confirm same elements\n",
    "    #Find distances and percent error\n",
    "    bonds_in_only_hf = [b for b in list(bd_hf.keys()) if b not in list(bd_pdb.keys())]\n",
    "    bonds_in_only_pdb = [b for b in list(bd_pdb.keys()) if b not in list(bd_hf.keys())]\n",
    "    if (len(bonds_in_only_hf) > 0) or (len(bonds_in_only_pdb) > 0):\n",
    "        print(\"BOND MISMATCH PROBLEM\")\n",
    "    pct_err_dict = {} #Dictionary of percent errors\n",
    "    pct_err_list = [] #List of percent errors\n",
    "    \n",
    "    #Find percent error of each bond\n",
    "    for bc in bd_pdb.keys():\n",
    "        #Element check\n",
    "        if (bd_hf[bc][\"elements\"][0] != bd_pdb[bc][\"elements\"][0]) or (bd_hf[bc][\"elements\"][1] != bd_pdb[bc][\"elements\"][1]):\n",
    "            print(\"ELEMENT MISMATCH PROBLEM,inference and pdb compare\")\n",
    "            print(bc)\n",
    "            print(\"HF\")\n",
    "            print(bd_hf[bc][\"elements\"])\n",
    "            print(\"PDB\")\n",
    "            print(bd_pdb[bc][\"elements\"])\n",
    "\n",
    "        #Percent error calc\n",
    "        dist_pdb = bd_pdb[bc][\"distance\"]\n",
    "        dist_hf = bd_hf[bc][\"distance\"]\n",
    "        dist_pct_error = 100.0 * (dist_hf - dist_pdb) / dist_pdb\n",
    "        pct_err_list.append(dist_pct_error)\n",
    "        pct_err_dict[bc] = {\"inf\" : dist_hf, \"pdb\" : dist_pdb, \"pe\" : dist_pct_error}\n",
    "        \n",
    "    #Now average\n",
    "    avg_bond_pe = np.average(pct_err_list)\n",
    "    \n",
    "    return pct_err_list, pct_err_dict, avg_bond_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8448a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Conformer generation\n",
    "#2/1/24 13:40 update to redo rdkit generation if under 10 conformers, try w/random coords 1st\n",
    "#Copied/slightly modified from https://github.com/HannesStark/EquiBind/blob/41bd00fd6801b95d2cf6c4d300cd76ae5e6dab5e/commons/process_mols.py#L447\n",
    "#Thank you (redact_for_anonymity) for advice!\n",
    "#Copied from ps down to conformers.append with some edits\n",
    "#Also relevant/consulted:\n",
    "#Ref https://iwatobipen.wordpress.com/2021/01/31/generate-conformers-script-with-rdkit-rdkit-chemoinformatics/\n",
    "#Ref https://greglandrum.github.io/rdkit-blog/posts/2023-02-04-working-with-conformers.html\n",
    "#Ref https://www.rdkit.org/docs/source/rdkit.Chem.rdDistGeom.html for random seed\n",
    "#Ref https://github.com/rdkit/rdkit/issues/2575, https://github.com/rdkit/rdkit/discussions/6489, https://github.com/rdkit/rdkit/discussions/4623\n",
    "#Ref https://github.com/rdkit/rdkit/discussions/5400 helped with figuring out a deterministic approach\n",
    "#I think I only need 1 seed because I am generating all conformers at once\n",
    "#Input:\n",
    "#(a) l_c_gen molecule for conformer generation, believe needs Hs\n",
    "#(b) cdir directory for saving conformer structures\n",
    "#(c) cname complex name\n",
    "#(d) count_conf how many conformers want\n",
    "#Output:\n",
    "#(a) conformers list of conformer coordinates\n",
    "############\n",
    "def generate_conformers(l_c_gen, \n",
    "                        cdir, \n",
    "                        cname, \n",
    "                        count_conf):\n",
    "\n",
    "    ps = AllChem.ETKDGv2()\n",
    "    \n",
    "    #Update code so it will be deterministic unless error and to add more attempts\n",
    "    #Ref https://github.com/rdkit/rdkit/issues/2536\n",
    "    #Ref https://www.rdkit.org/docs/source/rdkit.Chem.rdDistGeom.html#rdkit.Chem.rdDistGeom.EmbedParameters\n",
    "    ps.randomSeed = 5\n",
    "    ps.maxAttempts = 10\n",
    "    \n",
    "    ids = rdDistGeom.EmbedMultipleConfs(l_c_gen, count_conf, ps)\n",
    "    if (-1 in ids) or (len(list(ids)) < count_conf): #Edit here so this is now run if have under 10 conformers, 2/1/24 13:40\n",
    "        print('rdkit coords could not be generated without using random coords. using random coords now.')\n",
    "        ps.useRandomCoords = True\n",
    "        ids = rdDistGeom.EmbedMultipleConfs(l_c_gen, count_conf, ps)\n",
    "        AllChem.MMFFOptimizeMoleculeConfs(l_c_gen)\n",
    "    else:\n",
    "        AllChem.MMFFOptimizeMoleculeConfs(l_c_gen)\n",
    "    conformers = []\n",
    "\n",
    "    for i in range(count_conf):\n",
    "        conformers.append(l_c_gen.GetConformer(i).GetPositions())\n",
    "        \n",
    "    #Save if want to\n",
    "    #Copied writer code from https://stackoverflow.com/questions/69564484/how-to-save-rdkit-conformer-object-into-a-sdf-file\n",
    "    if cdir is not None:\n",
    "        cwriter = Chem.SDWriter(f\"{cdir}/{cname}_confs.sdf\")\n",
    "        for cid in range(l_c_gen.GetNumConformers()):\n",
    "            cwriter.write(l_c_gen, confId=cid)\n",
    "        \n",
    "    return conformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aee8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Obtain long range (LR) distances from dictionary\n",
    "#Input:\n",
    "#(a) d_lr_calc dictionary for LR distance calc\n",
    "#Output:\n",
    "#(a) lr_p_list list of LR pairs\n",
    "############\n",
    "def lr_list_from_dict(d_lr_calc):\n",
    "    \n",
    "    #Initialize list\n",
    "    lr_p_list = []\n",
    "    \n",
    "    #Each atom iterate\n",
    "    for a_lr_1 in d_lr_calc.keys():\n",
    "        \n",
    "        #Each further away atom relative to it iterate\n",
    "        for a_lr_2 in d_lr_calc[a_lr_1]:\n",
    "            \n",
    "            #Sorted pair\n",
    "            pair_sorted = sorted((a_lr_1, a_lr_2))\n",
    "            if pair_sorted not in lr_p_list:\n",
    "                lr_p_list.append(pair_sorted)\n",
    "                \n",
    "    return lr_p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c526061",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Work through bonds and find which pairs of atoms have 3+ bonds separating them\n",
    "#Note need to confirm Hs are always at the end of the molecule, I confirmed for the 268 test set entries analyzed\n",
    "#Input:\n",
    "#(a) l_find_lr ligand for finding longer-range pairs\n",
    "#(b) l_bonds list of ligand bonds\n",
    "#Output:\n",
    "#(a) lr_pairs list of pairs separated by 3+ bonds\n",
    "############\n",
    "def find_lr_pairs(l_find_lr, l_bonds):\n",
    "    lr_pairs = []\n",
    "    \n",
    "    #A. Find all heavy atoms\n",
    "    #Ref https://www.rdkit.org/docs/GettingStartedInPython.html\n",
    "    heavy_atom_list = []\n",
    "\n",
    "    #Record all heavy atoms\n",
    "    for a in l_find_lr.GetAtoms():\n",
    "        if a.GetAtomicNum() > 1:\n",
    "            heavy_atom_list.append(a.GetIdx())\n",
    "            \n",
    "    #B. Dictionary of bonded atoms for each atom\n",
    "    heavy_atom_bonds_dict = {}\n",
    "    for ah in heavy_atom_list:\n",
    "        \n",
    "        #Atom is key, value is list of bonded atoms\n",
    "        heavy_atom_bonds_dict[ah] = []\n",
    "        \n",
    "        #All bonds with atom- record other partner\n",
    "        for bcheck in l_bonds:\n",
    "            if ah in bcheck:\n",
    "                if bcheck[0] == ah:\n",
    "                    heavy_atom_bonds_dict[ah].append(bcheck[1])\n",
    "                if bcheck[1] == ah:\n",
    "                    heavy_atom_bonds_dict[ah].append(bcheck[0])\n",
    "    \n",
    "    #C Dictionary of 2 away for each atom\n",
    "    heavy_atom_2_away_dict = {}\n",
    "    \n",
    "    for ah2 in heavy_atom_list:\n",
    "        heavy_atom_2_away_dict[ah2] = []\n",
    "        \n",
    "        #Bonded neighbors\n",
    "        for ah2_bp in heavy_atom_bonds_dict[ah2]:\n",
    "            \n",
    "            #Their bonded neighbors\n",
    "            for ah2_bp_2 in heavy_atom_bonds_dict[ah2_bp]:\n",
    "                \n",
    "                #Do not add the atom itself and anything already in the bonded list or already in the 2 away list\n",
    "                if (ah2 != ah2_bp_2) and (ah2_bp_2 not in heavy_atom_bonds_dict[ah2]) and (ah2_bp_2 not in heavy_atom_2_away_dict[ah2]):\n",
    "                    heavy_atom_2_away_dict[ah2].append(ah2_bp_2)\n",
    "    \n",
    "    #D Dictionary of 3+ away for each atom\n",
    "    heavy_atom_lr_dict = {}\n",
    "    for ahlr in heavy_atom_list:\n",
    "        \n",
    "        #All 1 and 2 away plus atom itself\n",
    "        close_list = heavy_atom_bonds_dict[ahlr] + heavy_atom_2_away_dict[ahlr] + [ahlr]\n",
    "        \n",
    "        #This list is everything except for what is 1 or 2 away\n",
    "        heavy_atom_lr_dict[ahlr] = [hc for hc in heavy_atom_list if hc not in close_list]\n",
    "    \n",
    "    #E List from dictionary\n",
    "    lr_pairs = lr_list_from_dict(heavy_atom_lr_dict)\n",
    "    \n",
    "    return lr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Determine which distances have a low enough standard deviation to be consensus distances\n",
    "#Input\n",
    "#(a) conf_sum_d dictionary with information for each distance's value in the conformer ensemble\n",
    "#(b) consensus_sd_val threshold below which a distance is a consensus distance\n",
    "#Output\n",
    "#(a) consensus_d_list list of distances with low sds, representing consensus distances\n",
    "#(b) len(consensus_d_list) how many distances are consensus ones\n",
    "############\n",
    "def find_consensus_dist(conf_sum_d, consensus_sd_val):\n",
    "    consensus_d_list = []\n",
    "    \n",
    "    #Iterate over each distance, if sd is low enough add to list\n",
    "    for lr_dist_check_c in conf_sum_d.keys():\n",
    "        if conf_sum_d[lr_dist_check_c][\"sd\"] < consensus_sd_val:\n",
    "            consensus_d_list.append(lr_dist_check_c)\n",
    "            \n",
    "    return consensus_d_list, len(consensus_d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#2/1/24 19:30 centralize analysis of conformers\n",
    "#Generate conformers and determine consensus distances and % error relative to pdb\n",
    "#Input:\n",
    "#(a) l_for_conf ligand for conformer analysis - should have hydrogens ref https://greglandrum.github.io/rdkit-blog/posts/2023-02-04-working-with-conformers.html\n",
    "#(b) dir_c directory for results\n",
    "#(c) name_c name of complex\n",
    "#(d) c_bonds bonded atom list of conformer\n",
    "#(e) l_from_inference ligand from inference for % error calculation. It may not have Hs, so I run checks of atom numbering\n",
    "#(f) list_sd_thresholds list of SD thresholds for consensus\n",
    "#Output:\n",
    "#(a) dict_of_sd_threshold_results consensus distance information for each sd cutoff. In particular this includes:\n",
    "#(i) consensus_dist_count how many consensus distances exist\n",
    "#(ii) consensus_dist_pe_list_conf list of percent error of consensus distances- conformer average relative to pdb value\n",
    "#(iii) consensus_dist_pe_list_inf list of percent error of consensus distances- inference relative to pdb value\n",
    "#(iv) consensus_dist_pe_dict dictionary with percent error info\n",
    "#(b) all_pair_sd_list list of all pairs' standard deviations\n",
    "############\n",
    "def analyze_conformers(l_for_conf, \n",
    "                       dir_c, \n",
    "                       name_c, \n",
    "                       c_bonds, \n",
    "                       l_from_inference, \n",
    "                       list_sd_thresholds):\n",
    "    \n",
    "    if len(l_from_inference.GetConformers()) > 1:\n",
    "        print(\"HF conf PBM\")\n",
    "    if len(l_for_conf.GetConformers()) > 1:\n",
    "        print(\"PDB conf PBM\")\n",
    "    \n",
    "    ############\n",
    "    #A. Process structure\n",
    "    ############\n",
    "    #Find all heavy atom pairs 3 or more bonds away from each other\n",
    "    heavy_atom_pairs_lr = find_lr_pairs(l_for_conf, c_bonds)\n",
    "    \n",
    "    if len(l_for_conf.GetConformers()) > 1:\n",
    "        print(\"PDB conf PBM,for conf. gen.\")\n",
    "        \n",
    "    #Find PDB pose distances for each pair\n",
    "    pdb_lr_dist_dict = obtain_dist_data(l_for_conf.GetConformers()[0], \n",
    "                                        l_for_conf, \n",
    "                                        False, \n",
    "                                        heavy_atom_pairs_lr)\n",
    "    \n",
    "    ############\n",
    "    #B. Generate conformers\n",
    "    ############\n",
    "    conf_num = 10\n",
    "    conf = generate_conformers(l_for_conf, dir_c, name_c, conf_num)\n",
    "    if len(conf) < 10:\n",
    "        print(\"Issue:too few conformers\")\n",
    "     \n",
    "    ############\n",
    "    #C. Find long range distances for each conformer\n",
    "    ############\n",
    "    #Dictionary with conformers as key\n",
    "    #Each conformer has its own dictionary\n",
    "    #Values are the distance dictionaries from obtain_dist_data\n",
    "    conf_lr_dist_dict = {}\n",
    "    for c in range(len(conf)):\n",
    "        conf_lr_dist_dict[c] = obtain_dist_data(l_for_conf.GetConformers()[c], \n",
    "                                                l_for_conf, \n",
    "                                                False,\n",
    "                                                heavy_atom_pairs_lr)\n",
    "      \n",
    "    #Now create dictionary of summary stats for each long-range distance\n",
    "    #Keys- atom pairs\n",
    "    #Values- dictionaries with the distance for each conformer, plus the average and standard deviation\n",
    "    conformer_summary_dict = {}\n",
    "    \n",
    "    #Add on list of all sds to show distribution\n",
    "    all_pair_sd_list = []\n",
    "    \n",
    "    #Record info for each pair\n",
    "    for ha_pair_lr in heavy_atom_pairs_lr:\n",
    "        \n",
    "        #Set up tuple\n",
    "        ha_pair_sort = sorted(ha_pair_lr)\n",
    "        ha_pair_sort_0 = ha_pair_sort[0]\n",
    "        ha_pair_sort_1 = ha_pair_sort[1]\n",
    "        ha_pair_lr_key = (ha_pair_sort_0, ha_pair_sort_1)\n",
    "        \n",
    "        #Dictionary setup\n",
    "        conformer_summary_dict[ha_pair_lr_key] = {}\n",
    "        \n",
    "        #Find the distance of interest in each conformer\n",
    "        conf_dist_list = [conf_lr_dist_dict[c_add][ha_pair_lr_key][\"distance\"] for c_add in conf_lr_dist_dict.keys()]\n",
    "        \n",
    "        #Also confirm elements match\n",
    "        for c_el_check in conf_lr_dist_dict.keys(): #For each conformer\n",
    "            \n",
    "            #Find relevant elements\n",
    "            c_el_check_lr_elements = conf_lr_dist_dict[c_el_check][ha_pair_lr_key][\"elements\"]\n",
    "            \n",
    "            #Compare to elements to PDB\n",
    "            if (c_el_check_lr_elements[0] != pdb_lr_dist_dict[ha_pair_lr_key][\"elements\"][0]) or (c_el_check_lr_elements[1] != pdb_lr_dist_dict[ha_pair_lr_key][\"elements\"][1]):\n",
    "                print(\"ELEMENT MISMATCH PROBLEM,conformers and pdb compare\")\n",
    "                print(ha_pair_lr)\n",
    "                print(\"CONF\")\n",
    "                print(c_el_check_lr_elements)\n",
    "                print(\"PDB\")\n",
    "                print(pdb_lr_dist_dict[ha_pair_lr_key][\"elements\"])\n",
    "        \n",
    "        #Record whole list, find summary statistics\n",
    "        conformer_summary_dict[ha_pair_lr_key][\"distance_list\"] = conf_dist_list\n",
    "        conformer_summary_dict[ha_pair_lr_key][\"avg\"] = np.average(conf_dist_list)\n",
    "        sd_across_confs = np.std(conf_dist_list, ddof = 1)\n",
    "        conformer_summary_dict[ha_pair_lr_key][\"sd\"] = sd_across_confs\n",
    "        \n",
    "        all_pair_sd_list.append(sd_across_confs)\n",
    "    \n",
    "    ############\n",
    "    #D Find how many distances are consensus distances, find consensus distance % error for conformers\n",
    "    ############\n",
    "    #Find the consensus distances information- for different SD thresholds    \n",
    "    #To hold results at different thresholds\n",
    "    dict_of_sd_threshold_results = {}\n",
    "    for consensus_sd_threshold in list_sd_thresholds:\n",
    "        \n",
    "        dict_of_sd_threshold_results[consensus_sd_threshold] = {}\n",
    "        consensus_dist_list, consensus_dist_count = find_consensus_dist(conformer_summary_dict, consensus_sd_threshold)\n",
    "\n",
    "        #Find percent error of each consensus distance\n",
    "        consensus_dist_pe_dict = {}\n",
    "        consensus_dist_pe_list_conf = []\n",
    "        for cd in consensus_dist_list:\n",
    "            conf_consensus_val = conformer_summary_dict[cd][\"avg\"]\n",
    "            conf_consensus_sd = conformer_summary_dict[cd][\"sd\"]\n",
    "            pdb_consensus_val = pdb_lr_dist_dict[cd][\"distance\"]\n",
    "            pe_conf_pdb = 100.0 * (conf_consensus_val - pdb_consensus_val) / pdb_consensus_val\n",
    "            consensus_dist_pe_dict[cd] = {\"pdb\" : pdb_consensus_val,\n",
    "                                     \"conformers\" : conf_consensus_val,\n",
    "                                     \"pdb_conf_pe\" : pe_conf_pdb,\n",
    "                                     \"conf_sd\" : conf_consensus_sd}\n",
    "            consensus_dist_pe_list_conf.append(pe_conf_pdb)\n",
    "\n",
    "\n",
    "        ############\n",
    "        #E For each consensus distance also find % error for inference\n",
    "        ############    \n",
    "        #Run check to make sure atom numbering is ok- that Hs are just at the end, heavy atom ordering is the same\n",
    "        check_atoms(l_from_inference, l_for_conf)\n",
    "\n",
    "        #Find the consensus distance atom pairs' distance values in the inference output\n",
    "        inference_consensus_dist_dict = obtain_dist_data(l_from_inference.GetConformers()[0], \n",
    "                                                         l_from_inference, \n",
    "                                                         False,\n",
    "                                                         consensus_dist_list)\n",
    "\n",
    "        #List of consensus distance percent error values in inference pose\n",
    "        consensus_dist_pe_list_inf = []\n",
    "\n",
    "        #Repeat percent error calculation, but now compare inference- not conformer average- to PDB\n",
    "        for icd in inference_consensus_dist_dict.keys():\n",
    "\n",
    "            #Confirm elements are the same\n",
    "            inference_elements = inference_consensus_dist_dict[icd][\"elements\"]\n",
    "            pdb_elements = pdb_lr_dist_dict[icd][\"elements\"]\n",
    "            if (inference_elements[0] != pdb_elements[0]) or (inference_elements[1] != pdb_elements[1]):\n",
    "                print(\"ELEMENT MISMATCH PROBLEM,inference and pdb compare\")\n",
    "                print(icd)\n",
    "                print(\"INFERENCE\")\n",
    "                print(inference_elements)\n",
    "                print(\"PDB\")\n",
    "                print(pdb_elements)\n",
    "\n",
    "            #Find inference distance and compute percent error relative to PDB\n",
    "            inference_dist_value = inference_consensus_dist_dict[icd][\"distance\"]\n",
    "            pdb_for_inf_pe = consensus_dist_pe_dict[icd][\"pdb\"]\n",
    "            inf_pe_rel_to_pdb = 100.0 * (inference_dist_value - pdb_for_inf_pe) / pdb_for_inf_pe\n",
    "            consensus_dist_pe_dict[icd][\"inf\"] = inference_dist_value\n",
    "            consensus_dist_pe_dict[icd][\"pdb_inf_pe\"] = inf_pe_rel_to_pdb\n",
    "            consensus_dist_pe_list_inf.append(inf_pe_rel_to_pdb)\n",
    "\n",
    "        #Now record for this sd threshold\n",
    "        dict_of_sd_threshold_results[consensus_sd_threshold][\"consensus_dist_count\"] = consensus_dist_count\n",
    "        dict_of_sd_threshold_results[consensus_sd_threshold][\"consensus_dist_pe_dict\"] = consensus_dist_pe_dict\n",
    "        dict_of_sd_threshold_results[consensus_sd_threshold][\"consensus_dist_pe_list_conf\"] = consensus_dist_pe_list_conf\n",
    "        dict_of_sd_threshold_results[consensus_sd_threshold][\"consensus_dist_pe_list_inf\"] = consensus_dist_pe_list_inf\n",
    "  \n",
    "    return dict_of_sd_threshold_results, all_pair_sd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Scatterplot\n",
    "#(a) xdat x vals for points\n",
    "#(b) ydat y vals for points\n",
    "#(c) xlabel x axis label\n",
    "#(d) ylabel y axis label\n",
    "#(e) color_scatter color for plot\n",
    "#(f) dsc dir for scatterplot\n",
    "#(g) strsc string for scatterplot\n",
    "#########\n",
    "def create_scatter(xdat, \n",
    "                   ydat, \n",
    "                   xlabel, \n",
    "                   ylabel, \n",
    "                   color_scatter,\n",
    "                   dsc,\n",
    "                   strsc):\n",
    "    \n",
    "    fp, ap = plt.subplots()\n",
    "    plt.scatter(xdat, ydat, alpha = 0.7, color = color_scatter)\n",
    "    plt.xlabel(xlabel, fontsize = 24)\n",
    "    plt.ylabel(ylabel, fontsize = 24)\n",
    "    plt.xticks(fontsize = 24)\n",
    "    plt.yticks(fontsize = 24)\n",
    "    \n",
    "    #Thank you (redact_for_anonymity) for Spearman R advice!\n",
    "    #Ref https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "    #TODO remove p values or note not perfect for this context bcs have under 500 points- currently noting in text\n",
    "    spearman_r = scipy.stats.spearmanr(xdat, ydat)\n",
    "    spearman_r_sci_not = sci_notation(spearman_r[1])\n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n",
    "    #Ref https://www.adamsmith.haus/python/answers/how-to-print-a-number-in-scientific-notation-in-python\n",
    "    plt.text(1.02, 0.88, f\"Spearman R : {spearman_r[0]:.3f}\\np-value : {spearman_r_sci_not}\", transform=ap.transAxes, fontsize = 20)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/12050393/how-to-force-the-y-axis-to-only-use-integers\n",
    "    if xlabel in [\"Heavy Atom Count\", \"Rotatable Bonds\"]:\n",
    "        ap.xaxis.get_major_locator().set_params(integer=True)\n",
    "        \n",
    "    if ylabel in [\"Heavy Atom Count\", \"Rotatable Bonds\"]:\n",
    "        ap.yaxis.get_major_locator().set_params(integer=True)\n",
    "        \n",
    "    if xlabel == \"Radius of Gyration % Error\":\n",
    "        plt.xticks(fontsize = 19)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dsc}/Scatter_{strsc}.png\", bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59957560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Scatterplot- two y values\n",
    "#Ref https://matplotlib.org/stable/gallery/subplots_axes_and_figures/secondary_axis.html\n",
    "#Ref https://matplotlib.org/stable/gallery/subplots_axes_and_figures/two_scales.html\n",
    "#(a) xdat x vals for points\n",
    "#(b) ydat1 y vals for points 1st series\n",
    "#(c) ydat2 y vals for points 2nd series\n",
    "#(d) xlabel x axis label\n",
    "#(e) ylabel1 y axis label 1st series\n",
    "#(f) ylabel2 y axis label 2nd series\n",
    "#(g) color_scatter1 color for plot 1st series\n",
    "#(h) color_scatter2 color for plot 2nd series\n",
    "#(i) dsc2 dir for double scatter\n",
    "#(j) strsc2 string for double scatter\n",
    "#########\n",
    "def create_scatter_2_y(xdat, \n",
    "                       ydat1, \n",
    "                       ydat2, \n",
    "                       xlabel, \n",
    "                       ylabel1, \n",
    "                       ylabel2, \n",
    "                       color_scatter1, \n",
    "                       color_scatter2,\n",
    "                       dsc2,\n",
    "                       strsc2):\n",
    "    \n",
    "    fp, ap = plt.subplots()\n",
    "\n",
    "    #One plot\n",
    "    ap.scatter(xdat, ydat1, color = color_scatter1, alpha = 0.5)\n",
    "\n",
    "    #Second\n",
    "    ap2 = ap.twinx()\n",
    "    ap2.scatter(xdat, ydat2, color = color_scatter2, alpha = 0.5)\n",
    "\n",
    "    ap.set_xlabel(xlabel, fontsize = 24)\n",
    "    if ylabel1 == \"Radius of Gyration % Error\":\n",
    "        ylabel1_clean = \"Rad. of Gyr. % Error\"\n",
    "    ap.set_ylabel(ylabel1_clean, fontsize = 24, color = color_scatter1)\n",
    "    ap2.set_ylabel(ylabel2, fontsize = 24, color = color_scatter2)\n",
    "    ap.tick_params(labelsize = 20)\n",
    "    ap2.tick_params(labelsize = 20)\n",
    "    y1_spearman = scipy.stats.spearmanr(xdat, ydat1)\n",
    "    y1_spearman_p = sci_notation(y1_spearman[1])\n",
    "    y2_spearman = scipy.stats.spearmanr(xdat, ydat2)\n",
    "    y2_spearman_p = sci_notation(y2_spearman[1])\n",
    "    \n",
    "    #Tighter labels\n",
    "    if xlabel == \"Average Bond Distance % Error\":\n",
    "        xlabel_short = \"Avg. BD %E\"\n",
    "    if ylabel1 == \"Radius of Gyration % Error\":\n",
    "        ylabel1_short = \"Rg %E\"\n",
    "    if ylabel2 == \"RMSD (Å)\":\n",
    "        ylabel2_short = \"RMSD\"\n",
    "    plt.text(1.23, 0.50, f\"{xlabel_short} and {ylabel1_short}\\nSpearman R : {y1_spearman[0]:.3f}\\np-value : {y1_spearman_p}\\n{xlabel_short} and {ylabel2_short}\\nSpearman R : {y2_spearman[0]:.3f}\\np-value : {y2_spearman_p}\", transform=ap.transAxes, fontsize = 20)\n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dsc2}/Double_Scatter_{strsc2}.png\", bbox_inches = \"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09abc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Histogram\n",
    "#(a) hdat data for histogram\n",
    "#(b) hlabel label for axis\n",
    "#(c) hcolor color for plot\n",
    "#(d) hbininf info for bins- dictionary\n",
    "#(e) dhist dir for saving\n",
    "#(f) strhist string for histogram\n",
    "#########\n",
    "def create_histogram(hdat, \n",
    "                     hlabel, \n",
    "                     hcolor, \n",
    "                     hbininf,\n",
    "                     dhist,\n",
    "                     strhist):\n",
    "    hbins = np.arange(hbininf[\"min\"], hbininf[\"max\"], hbininf[\"bspace\"])\n",
    "    fhist, ahist = plt.subplots()\n",
    "    plt.hist(hdat,\n",
    "             bins = hbins,\n",
    "             color = hcolor)\n",
    "    \n",
    "    plt.text(1.02, 0.50, \"$μ$ \" + f\"{np.average(hdat):.1f}\\n\" + \"$σ$ \" + f\"{np.std(hdat,ddof = 1):.1f}\", transform = ahist.transAxes, fontsize = 20)\n",
    "\n",
    "    #Ref https://stackoverflow.com/questions/12050393/how-to-force-the-y-axis-to-only-use-integers\n",
    "    ahist.yaxis.get_major_locator().set_params(integer=True)\n",
    "    \n",
    "    #Ref https://www.delftstack.com/howto/matplotlib/how-to-place-legend-outside-of-the-plot-in-matplotlib/\n",
    "    #Ref https://stackoverflow.com/questions/25068384/bbox-to-anchor-and-loc-in-matplotlib\n",
    "    if hlabel == \"Radius of Gyration % Error\":\n",
    "        plt.xticks(fontsize = 19)\n",
    "    else:\n",
    "        plt.xticks(fontsize = 24)\n",
    "    plt.ylabel(\"Frequency\", fontsize = 24)\n",
    "    plt.yticks(fontsize = 24)\n",
    "    plt.xlabel(hlabel, fontsize = 24)\n",
    "    \n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvline.html\n",
    "    if hlabel in [\"Radius of Gyration % Error\", \"Bond Distance % Error\"]:\n",
    "        plt.axvline(0, linestyle = \"--\", color = [0.3, 0.3, 0.3])\n",
    "        \n",
    "    #x lim to 0 if count dist.s or SDs\n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlim.html\n",
    "    if hlabel in [\"Count Consensus Distances\", \"All Long-Range Dist. Std. Dev. (Å)\"]:\n",
    "        plt.xlim(left = 0)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dhist}/Histogram_{strhist}.png\", bbox_inches = \"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    \n",
    "    below_0 = [i for i in hdat if i < 0]\n",
    "    print(f\"{hlabel} has {len(hdat)} entries, {len(below_0)} below 0 for {100.0 * len(below_0) / len(hdat)}% below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Histogram with 2 entries\n",
    "#(a) hdat_list list of data for histogram\n",
    "#(b) hlabel label for axis\n",
    "#(c) hcolor_list list of colors for plot\n",
    "#(d) hbininf info for bins- dictionary\n",
    "#(e) hdat_labels_list labels for histogram\n",
    "#(f) norm_boolean whether to normalize- now do not have option- instead made separate function, but should merge\n",
    "#(g) dhist2 dir for saving\n",
    "#(h) strhist2 string for saving\n",
    "#########\n",
    "def create_histogram_2(hdat_list, \n",
    "                       hlabel, \n",
    "                       hcolor_list, \n",
    "                       hbininf, \n",
    "                       hdat_labels_list, \n",
    "                       norm_boolean,\n",
    "                       dhist2,\n",
    "                       strhist2):\n",
    "    hbins = np.arange(hbininf[\"min\"], hbininf[\"max\"], hbininf[\"bspace\"])\n",
    "    fhist, ahist = plt.subplots()\n",
    "    \n",
    "    stats_string = \"\"\n",
    "    \n",
    "    for hd_i, hdat_pl in enumerate(hdat_list):\n",
    "        plt.hist(hdat_pl,\n",
    "                 bins = hbins,\n",
    "                 color = hcolor_list[hd_i],\n",
    "                 label = hdat_labels_list[hd_i],\n",
    "                 alpha = 0.50)\n",
    "        stats_for_entry = f\"{hdat_labels_list[hd_i]}\\n\"+\"$μ$ \"+f\"{np.average(hdat_pl):.1f}\\n\"+\"$σ$ \"+f\"{np.std(hdat_pl,ddof = 1):.1f}\\n\"\n",
    "        stats_string += stats_for_entry\n",
    "    \n",
    "    #Assume 2 entries, t-test\n",
    "    #ref https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "    #TODO- is this the correct test?\n",
    "    t_test_2_hist = scipy.stats.ttest_ind(hdat_list[0], hdat_list[1])\n",
    "    t_test_2_hist_sci_not = sci_notation(t_test_2_hist[1])\n",
    "    if t_test_2_hist[1] > 0.0:\n",
    "        stats_string += f\"t test p-value {t_test_2_hist_sci_not}\"\n",
    "    else:\n",
    "        print(\"0 t test, not including\")\n",
    "    print(t_test_2_hist)\n",
    "    \n",
    "    plt.text(1.02, 0.27, stats_string, transform = ahist.transAxes, fontsize = 16)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/12050393/how-to-force-the-y-axis-to-only-use-integers\n",
    "    ahist.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    #Ref https://www.delftstack.com/howto/matplotlib/how-to-place-legend-outside-of-the-plot-in-matplotlib/\n",
    "    #Ref https://stackoverflow.com/questions/25068384/bbox-to-anchor-and-loc-in-matplotlib\n",
    "    plt.legend(bbox_to_anchor = (1.02, 1.00), loc = \"upper left\", fontsize = 15)\n",
    "    plt.xlabel(hlabel, fontsize = 24)\n",
    "    plt.ylabel(\"Frequency\", fontsize = 24)\n",
    "    plt.xticks(fontsize = 21)\n",
    "    plt.yticks(fontsize = 24)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dhist2}/Hist_2_{strhist2}.png\", bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.show()  \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Create 1 list from list of lists\n",
    "#Inputs:\n",
    "#(a) list_to_consolidate list of lists\n",
    "#Output:\n",
    "#(a) consolidated_list list with all entries in each input list\n",
    "#########\n",
    "def list_consolidate(list_to_consolidate):\n",
    "    \n",
    "    consolidated_list = []\n",
    "    \n",
    "    for l_add in list_to_consolidate:\n",
    "        consolidated_list += l_add\n",
    "    \n",
    "    return consolidated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0267f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Create plots\n",
    "#Input:\n",
    "#(a) df_plot dataframe to plot\n",
    "#(b) rlist list of entries to remove\n",
    "#(c) pltd dir for saving\n",
    "#(d) pltstri string with info\n",
    "#########\n",
    "def plot_data(df_plot, \n",
    "              rlist,\n",
    "              pltd,\n",
    "              pltstri):\n",
    "    \n",
    "    #########\n",
    "    #Process, setup\n",
    "    #########\n",
    "    #Main color, thank you (redact_for_anonymity)!\n",
    "    main_color = [0.8666666666666667, 0.5176470588235295, 0.3215686274509804]\n",
    "    conf_color = [0.45, 0.65, 0.95]\n",
    "    \n",
    "    #Remove entries do not want\n",
    "    #Filter removals\n",
    "    #Ref https://saturncloud.io/blog/how-to-select-rows-from-a-dataframe-based-on-list-values-in-a-column-in-pandas/\n",
    "    #Ref https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
    "    df_removals = df_plot[\"Complex_Name\"].isin(rlist)\n",
    "    print(\"what is removed?\")\n",
    "    print(df_plot[df_removals])\n",
    "    df_after_remove = df_plot[~df_removals]\n",
    "    \n",
    "    #Label, color setup\n",
    "    df_to_label_dict = {\"Heavy_Atom_Count\" : \"Heavy Atom Count\",\n",
    "                        \"RMSD\" : \"RMSD (Å)\",\n",
    "                        \"Rot_Bonds\" : \"Rotatable Bond Count\",\n",
    "                        \"Rg_Percent_Error\" : \"Radius of Gyration % Error\",\n",
    "                        \"Rg_Percent_Error_Abs_Val\" : \"|Radius of Gyration % Error|\",\n",
    "                        \"Bond_Distance_Percent_Error_List\" : \"Bond Distance % Error\",\n",
    "                        \"SD_0.10_Consensus_Distance_Count\" : \"Count Consensus Distances\",\n",
    "                        \"All_LR_SD\" : \"All Long-Range Dist. Std. Dev. (Å)\"}\n",
    "    \n",
    "    sc_color_plt_dict = {(\"Heavy_Atom_Count\", \"RMSD\") : main_color,\n",
    "                      (\"Rot_Bonds\", \"RMSD\") : main_color,\n",
    "                      (\"Rg_Percent_Error\", \"RMSD\") : main_color,\n",
    "                      (\"Rg_Percent_Error\", \"Heavy_Atom_Count\") : main_color,\n",
    "                      (\"Rg_Percent_Error_Abs_Val\", \"RMSD\") : main_color,\n",
    "                      (\"Rg_Percent_Error_Abs_Val\", \"Heavy_Atom_Count\") : main_color}\n",
    "    \n",
    "    #Construct Rg absolute value list\n",
    "    rg_abs_val_list = [abs(rg_pe) for rg_pe in list(df_after_remove[\"Rg_Percent_Error\"])]\n",
    "    \n",
    "    #########\n",
    "    #Scatterplots\n",
    "    #########\n",
    "    #Scatterplots of drivers\n",
    "    for dpair in [[\"Heavy_Atom_Count\", \"RMSD\"], [\"Rot_Bonds\", \"RMSD\"], [\"Rg_Percent_Error\", \"RMSD\"], [\"Rg_Percent_Error_Abs_Val\", \"RMSD\"], [\"Rg_Percent_Error\", \"Heavy_Atom_Count\"], [\"Rg_Percent_Error_Abs_Val\", \"Heavy_Atom_Count\"]]:\n",
    "        dp0 = dpair[0]\n",
    "        dp1 = dpair[1]\n",
    "        \n",
    "        #Obtain x and y lists\n",
    "        if dp0 != \"Rg_Percent_Error_Abs_Val\":\n",
    "            x_rem = df_after_remove[dp0]\n",
    "        else:\n",
    "            x_rem = rg_abs_val_list\n",
    "        y_rem = df_after_remove[dp1]\n",
    "        \n",
    "        #Plot\n",
    "        create_scatter(x_rem, \n",
    "                       y_rem, \n",
    "                       df_to_label_dict[dp0], \n",
    "                       df_to_label_dict[dp1], \n",
    "                       sc_color_plt_dict[(dp0, dp1)], \n",
    "                       pltd,\n",
    "                       f\"{pltstri}_{dp0}_{dp1}\")\n",
    "        \n",
    "    #Two y axis scatterplot\n",
    "    create_scatter_2_y(df_after_remove[\"Bond_Distance_Percent_Error_Average\"], \n",
    "                       df_after_remove[\"Rg_Percent_Error\"], \n",
    "                       df_after_remove[\"RMSD\"], \n",
    "                       \"Average Bond Distance % Error\", \n",
    "                       \"Radius of Gyration % Error\", \n",
    "                       \"RMSD (Å)\", \n",
    "                       [0.65, 0.65, 0.3], \n",
    "                       [0.17, 0.90, 0.25],\n",
    "                       pltd,\n",
    "                       f\"{pltstri}_BDPE_RgPE_RMSD\")\n",
    "    #########\n",
    "    #Histograms\n",
    "    #########\n",
    "    #Histogram setup\n",
    "    #Extra step for some lists, to make 1 list out of all the lists\n",
    "    consolidated_data_dict = {} #keys metrics, values consolidated lists\n",
    "    keys_consolidate_list = [\"Bond_Distance_Percent_Error_List\", \n",
    "                              \"All_LR_SD\", \n",
    "                              \"SD_0.10_Conformer_Consensus_Percent_Error\",\n",
    "                              \"SD_0.10_Inference_Consensus_Percent_Error\"]\n",
    "    for kconsolidate in keys_consolidate_list:\n",
    "        consolidated_data_dict[kconsolidate] = list_consolidate(list(df_after_remove[kconsolidate])) \n",
    "        print(kconsolidate)\n",
    "        print(f\"max {max(consolidated_data_dict[kconsolidate])} min {min(consolidated_data_dict[kconsolidate])}\")\n",
    "    \n",
    "    #And now general setup\n",
    "    hist_color_plt_dict = {\"Rg_Percent_Error\" : main_color,\n",
    "                           \"Bond_Distance_Percent_Error_List\" :  main_color,\n",
    "                           \"SD_0.10_Consensus_Distance_Count\" : conf_color,\n",
    "                           \"All_LR_SD\" : conf_color,\n",
    "                           \"SD_0.10_Conformer_Consensus_Percent_Error\" : conf_color,\n",
    "                           \"SD_0.10_Inference_Consensus_Percent_Error\" : main_color}\n",
    "    hist_bin_params = {\n",
    "                        \"Rg_Percent_Error\" : {\n",
    "                                                \"min\" : -60.0,\n",
    "                                                \"max\" : 20.0,\n",
    "                                                \"bspace\" : 2.0\n",
    "                                             },\n",
    "                        \"Bond_Distance_Percent_Error_List\" : {\n",
    "                                                            \"min\" : -86.0,\n",
    "                                                            \"max\" : 48.0, #was 160\n",
    "                                                            \"bspace\" : 2.0\n",
    "                                                         },\n",
    "                         \"SD_0.10_Consensus_Distance_Count\" : {\n",
    "                                                         \"min\" : 0,\n",
    "                                                         \"max\" : 280,\n",
    "                                                         \"bspace\" : 5\n",
    "                                                      },\n",
    "                        \"All_LR_SD\" : {\n",
    "                                         \"min\" : 0.00,\n",
    "                                         \"max\" : 6.00,\n",
    "                                         \"bspace\" : 0.20\n",
    "                                      }\n",
    "                        \n",
    "                      }\n",
    "    \n",
    "    #And create histogram\n",
    "    for hplot in [\"Rg_Percent_Error\", \"Bond_Distance_Percent_Error_List\", \"SD_0.10_Consensus_Distance_Count\", \"All_LR_SD\"]:\n",
    "        if hplot not in keys_consolidate_list:\n",
    "            h_rem = df_after_remove[hplot]\n",
    "            print(f\"{hplot} maxhistval {max(list(h_rem))} minhistval {min(list(h_rem))}\")\n",
    "            create_histogram(h_rem, \n",
    "                             df_to_label_dict[hplot], \n",
    "                             hist_color_plt_dict[hplot], \n",
    "                             hist_bin_params[hplot],\n",
    "                             pltd,\n",
    "                             f\"{pltstri}_{hplot}\")\n",
    "        else:\n",
    "            if hplot in [\"Bond_Distance_Percent_Error_List\", \"All_LR_SD\"]:\n",
    "                create_histogram(consolidated_data_dict[hplot],\n",
    "                                 df_to_label_dict[hplot], \n",
    "                                 hist_color_plt_dict[hplot], \n",
    "                                 hist_bin_params[hplot],\n",
    "                                 pltd,\n",
    "                                 f\"{pltstri}_{hplot}\")\n",
    "                \n",
    "    #Add on histogram with 2 entries\n",
    "    if len(consolidated_data_dict[\"SD_0.10_Conformer_Consensus_Percent_Error\"]) != len(consolidated_data_dict[\"SD_0.10_Inference_Consensus_Percent_Error\"]):\n",
    "        print(\"PROBLEM, SD_0.10_Conformer_Consensus_Percent_Error and SD_0.10_Inference_Consensus_Percent_Error are diff. lengths\")\n",
    "    \n",
    "    create_histogram_2([consolidated_data_dict[\"SD_0.10_Conformer_Consensus_Percent_Error\"], \n",
    "                        consolidated_data_dict[\"SD_0.10_Inference_Consensus_Percent_Error\"]],\n",
    "                       \"% error of non-bonded consensus dist.\\n(relative to PDB)\",\n",
    "                       [conf_color, main_color], \n",
    "                       {\"max\" : 30.0, \"min\" : -60.0, \"bspace\" : 0.5}, \n",
    "                       [\"Conformer Consensus\", \"HarmonicFlow\"], \n",
    "                       False,\n",
    "                       pltd,\n",
    "                       f\"{pltstri}_Consensus_Inf_PE\")\n",
    "    \n",
    "    #Different axis limits\n",
    "    create_histogram_2([consolidated_data_dict[\"SD_0.10_Conformer_Consensus_Percent_Error\"], \n",
    "                        consolidated_data_dict[\"SD_0.10_Inference_Consensus_Percent_Error\"]],\n",
    "                       \"% error of non-bonded consensus dist.\\n(relative to PDB)\",\n",
    "                       [conf_color, main_color], \n",
    "                       {\"max\" : 60.0, \"min\" : -80.0, \"bspace\" : 0.5}, \n",
    "                       [\"Conformer Consensus\", \"HarmonicFlow\"], \n",
    "                       False,\n",
    "                       pltd,\n",
    "                       f\"{pltstri}_Consensus_Inf_PE_All_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#From earlier workflow\n",
    "#Create normalized data\n",
    "#Ref https://stackoverflow.com/questions/53063231/matplotlib-pyplot-hist-wrong-normed-property?noredirect=1&lq=1\n",
    "#Ref https://numpy.org/doc/stable/reference/generated/numpy.histogram.html\n",
    "#Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "#Ref https://stackoverflow.com/questions/22241240/plot-a-histogram-such-that-the-total-height-equals-1\n",
    "#Ref https://stackoverflow.com/questions/18082536/numpy-histogram-normalized-with-specified-edges-python\n",
    "#Ref https://stackoverflow.com/questions/3866520/plot-a-histogram-such-that-bar-heights-sum-to-1-probability/16399202#16399202\n",
    "#Create histogram data to normalize\n",
    "#Input:\n",
    "#(a) dplot data to plot\n",
    "#(b) bplot bins to plot\n",
    "#Output:\n",
    "#(a) hist_for_pl_norm histogram for plot\n",
    "#########\n",
    "def gen_hist_data(dplot, bplot):\n",
    "    hist_for_pl, bins_for_pl = np.histogram(dplot, density = True, bins= bplot)\n",
    "    hist_for_pl_sum = hist_for_pl.sum()\n",
    "    hist_for_pl_norm = hist_for_pl / hist_for_pl_sum\n",
    "    return hist_for_pl_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Plot the 3 consensus count distributions on 1 axis\n",
    "#Now SD values fixed, should edit to be more flexible\n",
    "#Input:\n",
    "#(a) check_df_sd_consensus_pl dataframe for plots\n",
    "#(b) dhist3 directory to save\n",
    "#########\n",
    "def plt_consensus_hist(check_df_sd_consensus_pl, dhist3):\n",
    "    \n",
    "    #Setup\n",
    "    bins_consensus = np.arange(0, 350, 5)\n",
    "    alpha_pl = 0.50\n",
    "    fhist, ahist = plt.subplots()\n",
    "    \n",
    "    #Plot\n",
    "    plt.hist(check_df_sd_consensus_pl['SD_0.05_Consensus_Distance_Count'], \n",
    "             bins = bins_consensus, \n",
    "             alpha = alpha_pl, \n",
    "             label = \"0.05 Å cutoff\", \n",
    "             color = [0.45, 0.95, 0.65])\n",
    "    plt.hist(check_df_sd_consensus_pl['SD_0.10_Consensus_Distance_Count'], \n",
    "             bins = bins_consensus, \n",
    "             alpha = alpha_pl, \n",
    "             label = \"0.10 Å cutoff\", \n",
    "             color = [0.45, 0.65, 0.95])\n",
    "    plt.hist(check_df_sd_consensus_pl['SD_0.15_Consensus_Distance_Count'], \n",
    "             bins = bins_consensus, \n",
    "             alpha = alpha_pl, \n",
    "             label = \"0.15 Å cutoff\", \n",
    "             color = [0.95, 0.45, 0.65])\n",
    "\n",
    "    #Summary stats\n",
    "    sd_consensus_str = \"\"\n",
    "    for cval in [0.05, 0.10, 0.15]:\n",
    "        key_str_c = f'SD_{cval:.2f}_Consensus_Distance_Count'\n",
    "        sd_consensus_str += f\"{cval:.2f} Å \"+\"$μ$ \"+f\"{np.average(check_df_sd_consensus_pl[key_str_c]):.1f}\"+\" $σ$ \"+f\"{np.std(check_df_sd_consensus_pl[key_str_c],ddof = 1):.1f}\\n\"\n",
    "\n",
    "    plt.text(1.02, 0.27, sd_consensus_str, transform = ahist.transAxes, fontsize = 16)\n",
    "\n",
    "    #Ref https://www.delftstack.com/howto/matplotlib/how-to-place-legend-outside-of-the-plot-in-matplotlib/\n",
    "    #Ref https://stackoverflow.com/questions/25068384/bbox-to-anchor-and-loc-in-matplotlib\n",
    "    plt.legend(bbox_to_anchor = (1.02, 1.00), loc = \"upper left\", fontsize = 15)\n",
    "    plt.xlabel(\"Count Consensus Distance\", fontsize = 20)\n",
    "    plt.ylabel(\"Frequency\", fontsize = 20)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    \n",
    "    #x lim to 0 \n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlim.html\n",
    "    plt.xlim(left = 0)\n",
    "\n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dhist3}/Hist_3_Consensus.png\", bbox_inches = \"tight\")\n",
    "\n",
    "    plt.show()  \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#From earlier workflow\n",
    "#Test and training data histogram\n",
    "#Input:\n",
    "#(a) train_data training data vals\n",
    "#(b) test_data test data vals\n",
    "#(c) maxval max for bins\n",
    "#(d) minval min for bins\n",
    "#(e) binval increment for bins\n",
    "#(f) pltxlabel x label for plot\n",
    "#(g) dhist_tt directory for saving\n",
    "#(h) strhist_tt string for plot\n",
    "#########\n",
    "def histogram_train_test(train_data, \n",
    "                         test_data,\n",
    "                         maxval, \n",
    "                         minval, \n",
    "                         binval,\n",
    "                         pltxlabel,\n",
    "                         dhist_tt,\n",
    "                         strhist_tt):\n",
    "    \n",
    "    avalh = 0.3\n",
    "    hist_bins = np.arange(minval, maxval, binval)\n",
    "    train_data_h = gen_hist_data(train_data, hist_bins)\n",
    "    test_data_h = gen_hist_data(test_data, hist_bins)\n",
    "    \n",
    "    fhist, ahist = plt.subplots()\n",
    "    plt.hist(hist_bins[:-1],\n",
    "         bins = hist_bins,\n",
    "         weights = test_data_h,\n",
    "         color = [0.8666666666666667, 0.5176470588235295, 0.3215686274509804], #thank you (redact_for_anonymity) for the color!\n",
    "         alpha = avalh,\n",
    "         label = \"Test\")    \n",
    "    plt.hist(hist_bins[:-1],\n",
    "         bins = hist_bins,\n",
    "         weights = train_data_h,\n",
    "         color = [0.92, 0.06, 0.87],\n",
    "         alpha = avalh,\n",
    "         label = \"Train\")\n",
    "\n",
    "    \n",
    "    #Run t test\n",
    "    #Ref https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "    #TODO- is this the correct test?\n",
    "    t_test_tr_test = scipy.stats.ttest_ind(train_data, test_data)\n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n",
    "    #Ref https://www.adamsmith.haus/python/answers/how-to-print-a-number-in-scientific-notation-in-python\n",
    "    #Code for sci. notation\n",
    "    t_test_tr_test_sci_not = sci_notation(t_test_tr_test[1])\n",
    "    #Ref https://stackoverflow.com/questions/21226868/superscript-in-python-plots for plot\n",
    "    #Ref https://stackoverflow.com/questions/21226868/superscript-in-python-plots\n",
    "    plt.text(1.02, 0.10, f\"t test p-value:\\n{t_test_tr_test_sci_not}\\n\"+\"$μ_{train}$\"+f\" {np.average(train_data):.1f}\\n\"+\"$σ_{train}$\" + f\" {np.std(train_data,ddof = 1):.1f}\\n\"+\"$μ_{test}$\"+f\" {np.average(test_data):.1f}\\n\"+\"$σ_{test}$\"+f\" {np.std(test_data,ddof = 1):.1f}\", transform = ahist.transAxes, fontsize = 20)\n",
    "\n",
    "    #Ref https://www.delftstack.com/howto/matplotlib/how-to-place-legend-outside-of-the-plot-in-matplotlib/\n",
    "    plt.legend(bbox_to_anchor = (1.02, 1.00), loc = \"upper left\", fontsize = 20)\n",
    "    plt.xlabel(pltxlabel, fontsize = 24)\n",
    "    plt.ylabel(\"Norm. Frequency\", fontsize = 24)\n",
    "    plt.xticks(fontsize = 24)\n",
    "    plt.yticks(fontsize = 24)\n",
    "    \n",
    "    if pltxlabel == \"Heavy Atom Count\":\n",
    "        plt.xticks(fontsize = 20)\n",
    "    \n",
    "    #x lim to 0 \n",
    "    #Ref https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlim.html\n",
    "    plt.xlim(left = 0)\n",
    "    \n",
    "    #Ref https://stackoverflow.com/questions/55942693/how-do-i-save-the-entire-graph-without-it-being-cut-off\n",
    "    plt.savefig(f\"{dhist_tt}/Hist_Test_Train_{strhist_tt}.png\", bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Obtain list from a file\n",
    "#Input:\n",
    "#(a) f_name file name\n",
    "#Output:\n",
    "#(a) list_from_file list from the file\n",
    "#########\n",
    "def obtain_list_from_file(f_name):\n",
    "    list_from_file = []\n",
    "    file_for_list = open(f_name, \"r\")\n",
    "    for entry_add in file_for_list:\n",
    "        list_from_file.append(entry_add.rstrip())\n",
    "    file_for_list.close()\n",
    "    return list_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#Launch the analysis\n",
    "#Input:\n",
    "#(a) pdb_dir the directory with PDBbind data\n",
    "#(b) inf_dir directory with inference data\n",
    "#(c) dir_output the directory for output\n",
    "#(d) complex_list the list of complexes to analyze\n",
    "#(e) boolean_sanitize whether to sanitize molecules\n",
    "#Output\n",
    "#(a) pd_df output dataframe\n",
    "#########\n",
    "def run_analysis(pdb_dir,\n",
    "                 inf_dir,\n",
    "                 dir_output,\n",
    "                 complex_list,\n",
    "                 boolean_sanitize):\n",
    "\n",
    "    #Make directory\n",
    "    os.mkdir(dir_output)\n",
    "    c_dir_output = f\"{dir_output}/Conformers\"\n",
    "    os.mkdir(c_dir_output)\n",
    "    \n",
    "    #Job info\n",
    "    j_info = open(f\"{dir_output}/Job_info.txt\", \"w\")\n",
    "    j_info.write(f\"PDB : {pdb_dir}\\nInf: {inf_dir}\\nSanitize: {boolean_sanitize}\\nComplexes\\n\")\n",
    "    for co in complex_list:\n",
    "        j_info.write(f\"{co}\\n\")\n",
    "    j_info.close()\n",
    "    \n",
    "    #List if there are 2+ molecules in the file\n",
    "    mult_molec_in_file = []\n",
    "    \n",
    "    #Metrics for both only pdb (train) and inference + pdb (test) runs\n",
    "    list_pdb_names = [] #Complex names\n",
    "    list_hac = [] #Count of ligand heavy atoms\n",
    "    list_rot_bonds = [] #Count of rotatable bonds\n",
    "    list_rg_pdb = [] #PDB ligand radius of gyration\n",
    "    \n",
    "    #If this is a test set also use other mostly pose-based metrics/more in-depth analysis\n",
    "    #Some of the consensus analyses could also be run for entire training set but for consistency not yet doing\n",
    "    if inf_dir is not None:\n",
    "\n",
    "        #Lists will use for dataframe\n",
    "        list_rmsds = [] #RMSD- of inference 1 sample last entry in xt.pdb to PDBbind\n",
    "        list_rg_inf = [] #Inference pose radius of gyration\n",
    "        list_rg_pct_error = [] #Radius of gyration percent error\n",
    "        list_b_dist_pe_all = [] #Bond distance percent error list- each entry is a list of all percent errors for that complex\n",
    "        list_b_dist_pe_dict = [] #Bond distance percent error dictionary list- each entry is a dictionary with information on each bond used for its percent error calculation\n",
    "        list_b_dist_pe_avg = [] #Average bond distance percent error list- each entry is the average of all bonds' percent errors for that complex\n",
    "        list_lr_sd_all = [] #Every long range SD\n",
    "        \n",
    "        #Consensus results dictionary for each SD threshold\n",
    "        #Keys are the thresholds, values are dictionaries with the metrics\n",
    "        #Note right now there is repetition - a higher SD will also have the results for lower SD thresholds plus those in between the next lowest threshold and this threshold\n",
    "        #Definitely a point for further cleaning\n",
    "        sd_threshold_dict = {}\n",
    "        sd_threshold_dict_keys = [0.05, 0.10, 0.15]\n",
    "        for sd_t_use in sd_threshold_dict_keys:\n",
    "            sd_threshold_dict[sd_t_use] = {\n",
    "                                            \"list_consensus_dist_count\" : [], #How many consensus distances in each complex\n",
    "                                            \"list_consensus_pe_conf_list\" : [], #Percent errors of conformer consensus relative to PDB\n",
    "                                            \"list_consensus_pe_inf_list\" : [], #Percent errors of inference relative to PDB\n",
    "                                            \"list_consensus_dict\" : [] #Dictionaries with consensus distance info\n",
    "                                          }\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    #Check each complex\n",
    "    for ip, pdb_name in enumerate(complex_list):\n",
    "\n",
    "        #Progress track\n",
    "        print(f\"On {pdb_name} index {ip}\")\n",
    "        #Copied below from https://github.com/gcorso/DiffDock/blob/main/datasets/pdbbind.py\n",
    "        for file in os.listdir(os.path.join(pdb_dir, pdb_name)):\n",
    "            if file.endswith(\".sdf\") and 'rdkit' not in file:\n",
    "\n",
    "                lig = read_molecule(os.path.join(pdb_dir, pdb_name, file), remove_hs=False, sanitize=boolean_sanitize)\n",
    "                #Pull w/o H for rmsd comparison w/HF output\n",
    "                lig_no_h = read_molecule(os.path.join(pdb_dir, pdb_name, file), remove_hs=True, sanitize=boolean_sanitize)\n",
    "                if lig_no_h is None and os.path.exists(os.path.join(pdb_dir, pdb_name, file[:-4] + \".mol2\")):  # read mol2 file if sdf file cannot be sanitized\n",
    "                    print('Using the .sdf file failed. We found a .mol2 file instead and are trying to use that.')\n",
    "                    lig = read_molecule(os.path.join(pdb_dir, pdb_name, file[:-4] + \".mol2\"), remove_hs=False, sanitize=boolean_sanitize)\n",
    "                    lig_no_h = read_molecule(os.path.join(pdb_dir, pdb_name, file[:-4] + \".mol2\"), remove_hs=True, sanitize=boolean_sanitize)\n",
    "\n",
    "\n",
    "                #Parse to see if there are 2+ molecules. If so- then remove from analysis\n",
    "                #ref https://www.rdkit.org/docs/GettingStartedInPython.html#molecular-fragments\n",
    "                #ref https://www.daylight.com/dayhtml/doc/theory/theory.smiles.html\n",
    "                l_smiles = Chem.MolToSmiles(lig)\n",
    "                if \".\" in l_smiles:\n",
    "                    mult_molec_in_file.append(pdb_name)\n",
    "                    print(f\"PDB {pdb_name} Ligand {l_smiles} multiple molecules, do not use\")\n",
    "                    \n",
    "                #If 1 molecule continue w/analysis\n",
    "                else:\n",
    "                    #Record in list\n",
    "                    list_pdb_names.append(pdb_name)\n",
    "                    \n",
    "                    #Also read in inference pose if working with a test set\n",
    "                    #TODO does sanitize=False affect properties of interest? based on checks thus far no\n",
    "                    if inf_dir is not None:\n",
    "                        lighf = read_molecule(f\"{inf_dir}/{pdb_name}_x20.pdb\", remove_hs=True, sanitize=False)\n",
    "\n",
    "                        #Pull positions\n",
    "                        #Ref https://github.com/HannesStark/EquiBind/blob/41bd00fd6801b95d2cf6c4d300cd76ae5e6dab5e/commons/process_mols.py#L447 helped process molecules\n",
    "                        lig_coord = lig_no_h.GetConformer(0).GetPositions()\n",
    "                        hf_coord = lighf.GetConformer(0).GetPositions()\n",
    "\n",
    "                        #Confirm atom count same, sometimes Hs are included\n",
    "                        count_l_atoms = len(lig_no_h.GetAtoms())\n",
    "                        count_lhf_atoms = len(lighf.GetAtoms())\n",
    "\n",
    "                        #If Hs are included only use non-H atom coordinates\n",
    "                        if count_l_atoms != count_lhf_atoms:\n",
    "\n",
    "                            #New coordinate list, iterate over atoms, only record if not hydrogen\n",
    "                            lig_coord_cleaned = []\n",
    "                            for ac, cc in zip(lig_no_h.GetAtoms(), lig_coord):\n",
    "                                if ac.GetSymbol() != \"H\":\n",
    "                                    lig_coord_cleaned.append(list(cc))\n",
    "\n",
    "                        else:\n",
    "                            lig_coord_cleaned = lig_coord\n",
    "\n",
    "                        #RMSD calculation\n",
    "                        rmsd_calc = rmsd_calc_no_torch(lig_coord_cleaned, hf_coord)\n",
    "                        list_rmsds.append(rmsd_calc)\n",
    "                    else:\n",
    "                        lighf = None\n",
    "\n",
    "                    #Property calculation\n",
    "                    prop_calc = lprop_calc(lig_no_h, lighf)\n",
    "\n",
    "                    #Update lists\n",
    "                    list_hac.append(prop_calc[\"Heavy_Atom_Count\"])\n",
    "                    list_rot_bonds.append(prop_calc[\"Rot_Bonds\"])\n",
    "                    list_rg_pdb.append(prop_calc[\"Rg_PDB\"])\n",
    "\n",
    "                    #If working with test set update additional lists, run bond distance % error and consensus distance analysis\n",
    "                    if lighf is not None:\n",
    "                        list_rg_pct_error.append(prop_calc[\"Rg_Percent_Error\"])\n",
    "                        list_rg_inf.append(prop_calc[\"Rg_Inf\"])\n",
    "\n",
    "                        #Bond percent error\n",
    "                        bdist_l, bdist_d, bdist_avg = bond_percent_error_calc(lig_no_h, lighf)\n",
    "\n",
    "                        #Update lists\n",
    "                        list_b_dist_pe_all.append(bdist_l)\n",
    "                        list_b_dist_pe_dict.append(bdist_d)\n",
    "                        list_b_dist_pe_avg.append(bdist_avg)\n",
    "\n",
    "                        #Conformer analysis\n",
    "                        consensus_sd_output_dict, list_all_sd = analyze_conformers(lig, \n",
    "                                                                                   c_dir_output, \n",
    "                                                                                   pdb_name, \n",
    "                                                                                   list(bdist_d.keys()), \n",
    "                                                                                   lighf,\n",
    "                                                                                   list(sd_threshold_dict.keys()))\n",
    "                        \n",
    "                        #Have multiple thresholds, update each dictionary\n",
    "                        for sd_update in sd_threshold_dict.keys():\n",
    "                            sd_threshold_dict[sd_update][\"list_consensus_dist_count\"].append(consensus_sd_output_dict[sd_update][\"consensus_dist_count\"])\n",
    "                            sd_threshold_dict[sd_update][\"list_consensus_pe_conf_list\"].append(consensus_sd_output_dict[sd_update][\"consensus_dist_pe_list_conf\"])\n",
    "                            sd_threshold_dict[sd_update][\"list_consensus_pe_inf_list\"].append(consensus_sd_output_dict[sd_update][\"consensus_dist_pe_list_inf\"])\n",
    "                            sd_threshold_dict[sd_update][\"list_consensus_dict\"].append(consensus_sd_output_dict[sd_update][\"consensus_dist_pe_dict\"])\n",
    "                        \n",
    "                        #Update list of all LR SDs\n",
    "                        list_lr_sd_all.append(list_all_sd)\n",
    "\n",
    "    #Now construct dataframe\n",
    "    chem_prop_dict = {\n",
    "                        \"Complex_Name\" : list_pdb_names,\n",
    "                        \"Heavy_Atom_Count\" : list_hac,\n",
    "                        \"Rot_Bonds\" : list_rot_bonds,\n",
    "                        \"Rg_PDB\" : list_rg_pdb,\n",
    "                     }\n",
    "    \n",
    "    #Additional properties if working with inference output\n",
    "    if lighf is not None:\n",
    "        chem_prop_dict[\"RMSD\"] = list_rmsds\n",
    "        chem_prop_dict[\"Rg_Inf\"] = list_rg_inf\n",
    "        chem_prop_dict[\"Rg_Percent_Error\"] = list_rg_pct_error\n",
    "        chem_prop_dict[\"Bond_Distance_Percent_Error_List\"] = list_b_dist_pe_all\n",
    "        chem_prop_dict[\"Bond_Distance_Percent_Error_Dict\"] = list_b_dist_pe_dict\n",
    "        chem_prop_dict[\"Bond_Distance_Percent_Error_Average\"] = list_b_dist_pe_avg\n",
    "        chem_prop_dict[\"All_LR_SD\"] = list_lr_sd_all\n",
    "        \n",
    "        #Each SD threshold has its own entries\n",
    "        for sd_for_df in sd_threshold_dict.keys():\n",
    "            chem_prop_dict[f\"SD_{sd_for_df:.2f}_Consensus_Distance_Count\"] = sd_threshold_dict[sd_for_df][\"list_consensus_dist_count\"]\n",
    "            chem_prop_dict[f\"SD_{sd_for_df:.2f}_Conformer_Consensus_Percent_Error\"] = sd_threshold_dict[sd_for_df][\"list_consensus_pe_conf_list\"]\n",
    "            chem_prop_dict[f\"SD_{sd_for_df:.2f}_Inference_Consensus_Percent_Error\"] = sd_threshold_dict[sd_for_df][\"list_consensus_pe_inf_list\"]\n",
    "            chem_prop_dict[f\"SD_{sd_for_df:.2f}_Consensus_Info_Dict\"] = sd_threshold_dict[sd_for_df][\"list_consensus_dict\"]\n",
    "        \n",
    "\n",
    "    #Ref https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "    pd_df = pd.DataFrame(data = chem_prop_dict)\n",
    "    #Save out csv\n",
    "    #Ref https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "    pd_df.to_csv(f\"{dir_output}/Chem_Prop_Data.csv\")\n",
    "    \n",
    "    #Save out multiple molecule info\n",
    "    np.save(f\"{dir_output}/Multiple_Molec_List.npy\", mult_molec_in_file)\n",
    "\n",
    "    #Plot\n",
    "    if lighf is not None:\n",
    "        plot_data(pd_df, [], dir_output, \"From_Script_Run\")\n",
    "\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94606ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set analysis\n",
    "#This has processed PDBbind data\n",
    "#From https://github.com/HannesStark/FlowSite/tree/main Zenodo link\n",
    "pdb_dir_check = \"PDBbind_dir\"\n",
    "\n",
    "#This has parsed _xt.pdb output, the 20th and last set of coordinates for each file, in one directory\n",
    "#To obtain this, the raw _xt.pdb output should have its last set of coordinates extracted to a file titled {namepdb}_x20.pdb\n",
    "#All such files should be in one directory\n",
    "hf_out_dir_check = \"output_pose_dir\"\n",
    "\n",
    "#List of PDB ids of test set compounds\n",
    "test_list = obtain_list_from_file(\"test_set_list.txt\")\n",
    "\n",
    "#Whether to sanitize\n",
    "s_boolean = True\n",
    "\n",
    "#Output directory\n",
    "dir_output_run = \"analysis_output_dir\"\n",
    "check_df_sd_consensus_new_c_2 = run_analysis(pdb_dir_check, \n",
    "                                     hf_out_dir_check, \n",
    "                                     dir_output_run,\n",
    "                                     test_list,\n",
    "                                     s_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce630068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run for training set too\n",
    "#This has processed PDBbind data\n",
    "pdb_dir_check = \"PDBbind_dir\"\n",
    "\n",
    "#List of PDB ids of training set compounds\n",
    "train_set_list = obtain_list_from_file(\"training_set_list.txt\")\n",
    "\n",
    "#Output directory\n",
    "dir_output_run_train = f\"{dir_output_run}_Training_Set\"\n",
    "check_df_training = run_analysis(pdb_dir_check, \n",
    "                        None, \n",
    "                        dir_output_run_train,\n",
    "                        train_set_list,\n",
    "                        s_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check max/min\n",
    "print(f\"Heavy_Atom_Count max {max(list(check_df_training['Heavy_Atom_Count']))} {max(list(check_df_sd_consensus_new_c_2['Heavy_Atom_Count']))}\")\n",
    "print(f\"Rot_Bonds max {max(list(check_df_training['Rot_Bonds']))} {max(list(check_df_sd_consensus_new_c_2['Rot_Bonds']))}\")\n",
    "print(f\"Rg max {max(list(check_df_training['Rg_PDB']))} {max(list(check_df_sd_consensus_new_c_2['Rg_PDB']))}\")\n",
    "print(f\"SD consensus max {max(list(check_df_sd_consensus_new_c_2['SD_0.15_Consensus_Distance_Count']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d962575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output directory for additional plots\n",
    "dir_output_additional_plots = f\"{dir_output_run}_Additional_Plots\"\n",
    "os.mkdir(dir_output_additional_plots)\n",
    "\n",
    "#Test/train plots\n",
    "histogram_train_test(check_df_training[\"Heavy_Atom_Count\"], \n",
    "                         check_df_sd_consensus_new_c_2[\"Heavy_Atom_Count\"],\n",
    "                         170.0, \n",
    "                         0.0, \n",
    "                         2.0,\n",
    "                         \"Heavy Atom Count\",\n",
    "                         dir_output_additional_plots,\n",
    "                         \"Heavy_Atom_Count\")\n",
    "\n",
    "histogram_train_test(check_df_training[\"Rot_Bonds\"], \n",
    "                         check_df_sd_consensus_new_c_2[\"Rot_Bonds\"],\n",
    "                         60.0, \n",
    "                         0.0, \n",
    "                         1.0,\n",
    "                         \"Rotatable Bond Count\",\n",
    "                         dir_output_additional_plots,\n",
    "                         \"Rot_Bonds\")\n",
    "\n",
    "histogram_train_test(check_df_training[\"Rg_PDB\"], \n",
    "                         check_df_sd_consensus_new_c_2[\"Rg_PDB\"],\n",
    "                         14.0, \n",
    "                         0.0, \n",
    "                         0.2,\n",
    "                         \"Radius of Gyration\",\n",
    "                         dir_output_additional_plots,\n",
    "                         \"Rg_PDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9af2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consensus distances for different thresholds\n",
    "plt_consensus_hist(check_df_sd_consensus_new_c_2, \n",
    "                   dir_output_additional_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c90642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain count of complexes with 5+ consensus distances\n",
    "print(len([i for i in check_df_sd_consensus_new_c_2[\"SD_0.10_Consensus_Distance_Count\"] if i > 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max min check\n",
    "consol_05 = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.05_Conformer_Consensus_Percent_Error\"]))\n",
    "consol_10 = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.10_Conformer_Consensus_Percent_Error\"]))\n",
    "consol_15 = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.15_Conformer_Consensus_Percent_Error\"]))\n",
    "print(\"conformer consensus\")\n",
    "print(f\"consol_05 max {max(consol_05)} min {min(consol_05)} len {len(consol_05)}\")\n",
    "print(f\"consol_10 max {max(consol_10)} min {min(consol_10)} len {len(consol_10)}\")\n",
    "print(f\"consol_15 max {max(consol_10)} min {min(consol_15)} len {len(consol_15)}\")\n",
    "\n",
    "#Inference max min check\n",
    "consol_05_i = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.05_Inference_Consensus_Percent_Error\"]))\n",
    "consol_10_i = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.10_Inference_Consensus_Percent_Error\"]))\n",
    "consol_15_i = list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.15_Inference_Consensus_Percent_Error\"]))\n",
    "print(\"inference\")\n",
    "print(f\"consol_05 max {max(consol_05_i)} min {min(consol_05_i)} len {len(consol_05_i)}\")\n",
    "print(f\"consol_10 max {max(consol_10_i)} min {min(consol_10_i)} len {len(consol_10_i)}\")\n",
    "print(f\"consol_15 max {max(consol_10_i)} min {min(consol_15_i)} len {len(consol_15_i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different threshold percent errors\n",
    "maxvalhpe = 30.0\n",
    "minvalhpe = -60.0\n",
    "ccolor = [0.45, 0.65, 0.95] \n",
    "icolor = [0.8666666666666667, 0.5176470588235295, 0.3215686274509804]\n",
    "create_histogram_2([list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.05_Conformer_Consensus_Percent_Error\"])), \n",
    "                list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.05_Inference_Consensus_Percent_Error\"]))],\n",
    "               \"% error of non-bonded consensus dist.\\n(relative to PDB, 0.05 Å cutoff)\",\n",
    "               [ccolor, icolor], \n",
    "               {\"max\" : maxvalhpe, \"min\" : minvalhpe, \"bspace\" : 0.5}, \n",
    "               [\"Conformer Consensus\", \"HarmonicFlow\"], \n",
    "               False,\n",
    "               dir_output_additional_plots,\n",
    "               \"0.05A_Consensus_Inf_PE\")\n",
    "\n",
    "create_histogram_2([list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.15_Conformer_Consensus_Percent_Error\"])), \n",
    "                list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.15_Inference_Consensus_Percent_Error\"]))],\n",
    "               \"% error of non-bonded consensus dist.\\n(relative to PDB, 0.15 Å cutoff)\",\n",
    "               [ccolor, icolor], \n",
    "               {\"max\" : maxvalhpe, \"min\" : minvalhpe, \"bspace\" : 0.5}, \n",
    "               [\"Conformer Consensus\", \"HarmonicFlow\"], \n",
    "               False,\n",
    "               dir_output_additional_plots,\n",
    "               \"0.15A_Consensus_Inf_PE\")\n",
    "\n",
    "create_histogram_2([list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.10_Conformer_Consensus_Percent_Error\"])), \n",
    "                list_consolidate(list(check_df_sd_consensus_new_c_2[\"SD_0.10_Inference_Consensus_Percent_Error\"]))],\n",
    "               \"% error of non-bonded consensus dist.\\n(relative to PDB, 0.10 Å cutoff)\",\n",
    "               [ccolor, icolor], \n",
    "               {\"max\" : maxvalhpe, \"min\" : minvalhpe, \"bspace\" : 0.5}, \n",
    "               [\"Conformer Consensus\", \"HarmonicFlow\"], \n",
    "               False,\n",
    "               dir_output_additional_plots,\n",
    "               \"0.10A_Consensus_Inf_PE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca017a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowsite",
   "language": "python",
   "name": "flowsite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
